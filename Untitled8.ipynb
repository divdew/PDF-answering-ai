{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hfBncQNcdTs",
        "outputId": "a7b4efb2-982b-41af-94a5-23c604bef6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.5-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.3 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "JbmcXt4gct7x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUmdMJKNc7Q4",
        "outputId": "22fb7bd4-5830-4b35-ac6a-de4157d8fbab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "def train_word2vec_model(tokens):\n",
        "    model = Word2Vec([tokens], vector_size=100, window=5, min_count=1, workers=4)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "cJv2QqH8c-p4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_sentence_embedding(sentence, model):\n",
        "    words = sentence.split()\n",
        "    embeddings = [model.wv[word] for word in words if word in model.wv]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "def answer_query(query, text, model):\n",
        "    sentences = text.split('.')\n",
        "    query_embedding = get_sentence_embedding(query, model)\n",
        "\n",
        "    best_score = -1\n",
        "    best_sentence = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_embedding = get_sentence_embedding(sentence, model)\n",
        "        score = cosine_similarity([query_embedding], [sentence_embedding])[0][0]\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_sentence = sentence\n",
        "\n",
        "    return best_sentence\n"
      ],
      "metadata": {
        "id": "sJBqyV54dCbP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(pdf_path, query):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    tokens = preprocess_text(text)\n",
        "    model = train_word2vec_model(tokens)\n",
        "    answer = answer_query(query, text, model)\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "\n",
        "#pdf_path = 'enter pdf file path'\n",
        "#query = \"give your question\"\n",
        "#answer is printed\n",
        "\n",
        "pdf_path = '/content/Untitled document.pdf'\n",
        "query = \"What is the primary goal of the document?\"\n",
        "answer = main(pdf_path, query)\n",
        "print(\"Answer:\", answer)\n",
        "\n",
        "query = \"what is architecture used\"\n",
        "answer = main(pdf_path, query)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjZPGvXLdIs0",
        "outputId": "c7e18191-9dcc-4ace-a346-e80e22787de6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  The primary goal is to train the \n",
            "model on a dataset of noisy and clean images, evaluate its \n",
            "performance using metrics such as Mean Squared Error (MSE), Peak \n",
            "Signal-to-Noise Ratio (PSNR), , and produce denoised images for a \n",
            "test set\n",
            "Answer: \n",
            "Architecture Used\n",
            "The architecture used in this project is a simple yet effective CNN \n",
            "designed specifically for denoising tasks\n"
          ]
        }
      ]
    }
  ]
}